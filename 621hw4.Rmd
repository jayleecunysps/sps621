---
title: "DATA 621 Homework 4"
author: "Jay Lee and Tyler Baker"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r include=FALSE}
# load libary
library(reshape)
library(car)
library(mice)
library(naniar)
library(readr)
library(dplyr)
library(knitr)
library(tidyr)
library(stringr)
library(extraoperators)
library(palmerpenguins)
library(psych)
library(ggplot2)
library(tidyverse)
library(pscl)

```


## Overview

In this homework assignment, we will explore, analyze and model a data set containing approximately 8000
records representing a customer at an auto insurance company. Each record has two response variables. The
first response variable, TARGET_FLAG, is a 1 or a 0. A “1” means that the person was in a car crash. A zero
means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero
if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero.
Our objective is to build multiple linear regression and binary logistic regression models on the training data
to predict the probability that a person will crash their car and also the amount of money it will cost if the person
does crash their car. We can only use the variables given to you (or variables that you derive from the variables
provided).


## Assignment Preparation:

For this assignment, the data which is in csv format is available on blackboard. I have couple questions need to think about it before starting the project like HW1. These questions give me the direction.

-Who is my audience for this time?
-Will anyone audit the report?
-What is the visual planning process?
-Is the data contain sensitive information?
-Any VARIABLE is not useful?
-Do we have data definition specification?

we decided to upload both crime evaluation data and training data to GitHub so that audience or auditor can access while looking at our report since the data has no sensitive info. This allow audience to review, explore and exchange ideas to myself or other party in the future instead of just reading it. Also, we both agree that it is easier to get people understanding the result if we include visual presentation. 

## Table Creation

First of all, we are loading both csv file into R studio. It is very important to review and study the data beforehand instead of just loading it. Understanding the data is the foundation of any reports. It is very difficult to provide useful insight if you do not even understand how the data works. We often need another professional to explain the data when it is needed. In fact, good data set is not easy to find, so checking the data is a must. 

After loading the data, we can see evaluation data has 2141 observation and 26 variable, and training has 8161 observation and 26 variable. It is a big data compare with the one we handle previously. The first question in my mind is how we handle the index. Should we keep the index? Will the index be the important element in between these 2 table? After careful consideration, we decided to delete the index based on the experience from previous projects. We will use ins_test_df table for the rest of the project. Deleting not useful data is something people always forget or miss. It is helpful for audience to understand your data by taking away the useless data to create a nice and clean data set.


```{r echo=FALSE}
# load data

ins_eval_df <- "https://raw.githubusercontent.com/jayleecunysps/sps621/main/insurance-evaluation-data.csv"
ins_eval_df <-read.csv(ins_eval_df)

ins_test_df <- "https://raw.githubusercontent.com/jayleecunysps/sps621/main/insurance_training_data.csv"
ins_test_df <-read.csv(ins_test_df)
ins_test_df <-select(ins_test_df,-1)


```

## Data Exploration

After creating the table, we would like to explore the data to gain more knowledge on it as well as learn what limitation we can face in the later part of the project!
First of all, I want to check what type the variables are. Sometimes wrong type of data crate issue, for example you do not want the money in type of character. You want it to be numeric or integer. If we do not check and explore it at first, it may delay the project timeline.
For this project, we use **function str** to check the internal structure of the table/list, 


```{r echo=FALSE}
str(ins_test_df)
```

this function str is used for compactly displaying the internal structure of the table so we can learn about the object and its constituents. As we can see it is all int and chr which is good! I am not seeing NA for now. however, we will double check later with summary!

We can see the table is creating by type of num and int.

We would like to understand the statistics behind of the data that we load to see if it is suitable to use for modeling. How do we do it? We are another R function called **summary**. It can show the mean, median and standard deviation.

From the summary, we can see Age, YOJ, CAR_AGE has NA.

Also we would love to see the result of target column since it is where we can tell whether the TARGET_FLAG is above the median TARGET_FLAG (1) or not (0) (response variable).



```{r echo=FALSE}
summary(ins_test_df)

mean(ins_test_df$TARGET_FLAG)

targetastext = as.character(ins_test_df$TARGET_FLAG)

data_target <- as.data.frame(table(ins_test_df$TARGET_FLAG))
data_target

ggplot(data_target, aes(x = Var1, y = Freq, fill = Var1)) +  # Plot with values on top
  geom_bar(stat = "identity") +
  geom_text(aes(label = Freq), vjust = 0)


```

We can see the data has NAs which needs some steps to handle it for later. NAs can be problematic in data modeling. We usually prefer complete set of the data. 

```{r echo=FALSE}

ins_test_df_box <- dplyr::select_if(ins_test_df, is.numeric)

ggplot(stack(ins_test_df_box), aes(x = ind, y = values)) + 
  geom_boxplot() +
  coord_cartesian(ylim = c(0, 120000)) +
  ggtitle("plot of ylim set as 120k") +
  theme(axis.text.x=element_text(angle=45, hjust=1)) 


ggplot(stack(ins_test_df_box), aes(x = ind, y = values)) + 
  geom_boxplot() +
  coord_cartesian(ylim = c(0, 1500)) +
  ggtitle("plot of ylim set as 1.5k") +
  theme(axis.text.x=element_text(angle=45, hjust=1)) 


ggplot(stack(ins_test_df_box), aes(x = ind, y = values)) + 
  geom_boxplot() +
  coord_cartesian(ylim = c(0, 150)) +
  ggtitle("plot of ylim set as 150") +
  theme(axis.text.x=element_text(angle=45, hjust=1)) 

```



```{r echo=FALSE}
ins_test_df_cor <- dplyr::select_if(ins_test_df, is.numeric)
cor(drop_na(ins_test_df_cor))
pairs.panels(ins_test_df_cor[1:11])
```
 
One thing catch my attention,correlations between tax and rad is 0.91 which is highly correlated.we may consider to delete one later.

The next step we try to understanding if what rad and tax column means.
 
rad: index of accessibility to radial highways (predictor variable)
tax: full-value property-tax rate per $10,000 (predictor variable)
 
It is logical to have highly correlated relation since it could mean that location is wealthy, so we decied to keep both column.
 
up next we would like to see the density of the data.



```{r echo=FALSE}

potdata = melt(ins_test_df)

ggplot(potdata, aes(x=value)) +
  ggtitle("Checking density of the data") +
  geom_density(fill='blue') + facet_wrap(~variable, scales ='free')

```
 We can see the instantaneous rate of the distribution of the numeric variable change. As we can see, we can find bimodal, normal and skewed distribution shapes. let me list out couple sample of each shape.
 
 **Normal: rm**
 
 **skewed: zn, chas, nox, age, dis, lstat** 
 
 **bimodal: tax, target** 

```{r}

gg_miss_upset(ins_test_df,nsets=3, nintersects = 50)
```


```{r echo=FALSE}
ggplot(ins_test_df,
       aes(x = YOJ,
           y = CAR_AGE)) +
  geom_miss_point()
```




```{r echo=FALSE}

ins_test_dfmean <- complete(mice(data = ins_test_df, m = 1, method = "mean"))
summary(ins_test_dfmean)
potdata2 = melt(ins_test_dfmean)


ggplot(potdata, aes(x=value)) +
  ggtitle("Checking density of the data before replacing NA") +
  geom_density(fill='blue') + facet_wrap(~variable, scales ='free')

ggplot(potdata2, aes(x=value)) +
  ggtitle("Checking density of the data after replacing NA") +
  geom_density(fill='blue') + facet_wrap(~variable, scales ='free')

```

#building model

```{r}
lm_model1 <- lm(TARGET_AMT ~., data = ins_test_dfmean)
summary(lm_model1)
```

